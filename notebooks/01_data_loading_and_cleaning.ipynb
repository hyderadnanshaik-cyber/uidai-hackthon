{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UIDAI Biometric Update Analysis - Data Loading and Cleaning\n",
    "\n",
    "**UIDAI Data Hackathon 2026**  \n",
    "**Project**: Backend Data Analytics - Biometric Update Patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Purpose\n",
    "\n",
    "This notebook performs **Step 1** of the analysis pipeline:\n",
    "1. Load Aadhaar enrolment and update datasets\n",
    "2. Perform initial data exploration\n",
    "3. Clean and preprocess the data\n",
    "4. Create age groups for demographic analysis\n",
    "5. Save cleaned datasets for further analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outputs\n",
    "- Cleaned enrolment dataset with age groups\n",
    "- Cleaned update dataset with quality categories\n",
    "- Data quality report\n",
    "- Saved processed files in `data/processed/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from scripts.data_loader import (\n",
    "    load_enrolment_data,\n",
    "    load_update_data,\n",
    "    get_data_info,\n",
    "    convert_date_columns,\n",
    "    save_processed_data\n",
    ")\n",
    "\n",
    "from scripts.data_cleaner import (\n",
    "    create_age_groups,\n",
    "    handle_missing_values,\n",
    "    standardize_categorical_columns,\n",
    "    remove_duplicates,\n",
    "    create_biometric_quality_categories,\n",
    "    get_cleaning_summary\n",
    ")\n",
    "\n",
    "# Configure display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All libraries and modules imported successfully\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets\n",
    "\n",
    "**Note**: Replace the file paths below with your actual UIDAI dataset files.  \n",
    "If you don't have the datasets yet, we'll create sample data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "# TODO: Replace these with your actual dataset file paths\n",
    "ENROLMENT_FILE = '../data/raw/enrolment_data.csv'\n",
    "UPDATE_FILE = '../data/raw/update_data.csv'\n",
    "\n",
    "# Check if files exist\n",
    "enrolment_path = Path(ENROLMENT_FILE)\n",
    "update_path = Path(UPDATE_FILE)\n",
    "\n",
    "if enrolment_path.exists() and update_path.exists():\n",
    "    print(\"✓ Dataset files found\")\n",
    "    USE_SAMPLE_DATA = False\n",
    "else:\n",
    "    print(\"⚠ Dataset files not found. Will create sample data for demonstration.\")\n",
    "    USE_SAMPLE_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Sample Data (if needed)\n",
    "\n",
    "This section creates realistic sample Aadhaar data for testing the pipeline.  \n",
    "**Skip this if you have actual UIDAI datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SAMPLE_DATA:\n",
    "    print(\"Creating sample datasets...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Sample size\n",
    "    n_enrolments = 50000\n",
    "    n_updates = 15000\n",
    "    \n",
    "    # Create sample enrolment data\n",
    "    states = ['Maharashtra', 'Uttar Pradesh', 'Bihar', 'West Bengal', 'Madhya Pradesh', \n",
    "              'Tamil Nadu', 'Rajasthan', 'Karnataka', 'Gujarat', 'Andhra Pradesh']\n",
    "    genders = ['Male', 'Female', 'Transgender']\n",
    "    center_types = ['Permanent', 'Temporary', 'Mobile', 'Bank', 'Post Office']\n",
    "    \n",
    "    df_enrolment = pd.DataFrame({\n",
    "        'Enrolment_ID': range(1, n_enrolments + 1),\n",
    "        'Enrolment_Date': pd.date_range('2015-01-01', periods=n_enrolments, freq='H'),\n",
    "        'State': np.random.choice(states, n_enrolments),\n",
    "        'Age': np.random.choice(range(0, 100), n_enrolments, p=[\n",
    "            0.02 if i < 6 else \n",
    "            0.015 if i < 19 else \n",
    "            0.025 if i < 41 else \n",
    "            0.015 if i < 61 else \n",
    "            0.008 for i in range(100)\n",
    "        ]),\n",
    "        'Gender': np.random.choice(genders, n_enrolments, p=[0.51, 0.485, 0.005]),\n",
    "        'Center_Type': np.random.choice(center_types, n_enrolments, p=[0.4, 0.25, 0.15, 0.12, 0.08]),\n",
    "        'Biometric_Quality_Score': np.random.randint(30, 100, n_enrolments)\n",
    "    })\n",
    "    \n",
    "    # Create sample update data\n",
    "    update_types = ['Biometric', 'Demographic', 'Address', 'Mobile', 'Email']\n",
    "    update_reasons = ['Error Correction', 'Life Event', 'Biometric Degradation', 'Document Change']\n",
    "    \n",
    "    df_updates = pd.DataFrame({\n",
    "        'Update_ID': range(1, n_updates + 1),\n",
    "        'Enrolment_ID': np.random.choice(df_enrolment['Enrolment_ID'], n_updates),\n",
    "        'Update_Date': pd.date_range('2018-01-01', periods=n_updates, freq='2H'),\n",
    "        'Update_Type': np.random.choice(update_types, n_updates, p=[0.35, 0.25, 0.20, 0.15, 0.05]),\n",
    "        'Update_Reason': np.random.choice(update_reasons, n_updates, p=[0.25, 0.30, 0.30, 0.15]),\n",
    "        'Previous_State': np.random.choice(states, n_updates),\n",
    "        'New_State': np.random.choice(states, n_updates)\n",
    "    })\n",
    "    \n",
    "    # Save sample data\n",
    "    Path('../data/raw').mkdir(parents=True, exist_ok=True)\n",
    "    df_enrolment.to_csv('../data/raw/enrolment_data.csv', index=False)\n",
    "    df_updates.to_csv('../data/raw/update_data.csv', index=False)\n",
    "    \n",
    "    print(f\"✓ Created sample enrolment data: {len(df_enrolment):,} records\")\n",
    "    print(f\"✓ Created sample update data: {len(df_updates):,} records\")\n",
    "    print(\"✓ Saved to data/raw/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Data Using Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enrolment data\n",
    "df_enrolment = load_enrolment_data('../data/raw/enrolment_data.csv')\n",
    "\n",
    "# Get data information\n",
    "enrolment_info = get_data_info(df_enrolment, \"Enrolment Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load update data\n",
    "df_updates = load_update_data('../data/raw/update_data.csv')\n",
    "\n",
    "# Get data information\n",
    "update_info = get_data_info(df_updates, \"Update Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of enrolment data\n",
    "print(\"Enrolment Data - First 5 Rows:\")\n",
    "display(df_enrolment.head())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(df_enrolment.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of update data\n",
    "print(\"Update Data - First 5 Rows:\")\n",
    "display(df_updates.head())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(df_updates.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "Now we'll clean the datasets by:\n",
    "1. Converting date columns to proper datetime format\n",
    "2. Handling missing values\n",
    "3. Removing duplicates\n",
    "4. Standardizing categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Convert Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert enrolment date column\n",
    "df_enrolment = convert_date_columns(df_enrolment, ['Enrolment_Date'])\n",
    "\n",
    "# Convert update date column\n",
    "df_updates = convert_date_columns(df_updates, ['Update_Date'])\n",
    "\n",
    "print(\"\\n✓ Date columns converted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and handle missing values in enrolment data\n",
    "df_enrolment_clean = handle_missing_values(df_enrolment, strategy='report')\n",
    "\n",
    "# If you want to actually handle missing values, use one of these strategies:\n",
    "# df_enrolment_clean = handle_missing_values(df_enrolment, strategy='drop_cols', threshold=0.3)\n",
    "# df_enrolment_clean = handle_missing_values(df_enrolment, strategy='fill_median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and handle missing values in update data\n",
    "df_updates_clean = handle_missing_values(df_updates, strategy='report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate enrolment records\n",
    "df_enrolment_clean = remove_duplicates(df_enrolment_clean, subset=['Enrolment_ID'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate update records\n",
    "df_updates_clean = remove_duplicates(df_updates_clean, subset=['Update_ID'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Standardize Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize categorical columns in enrolment data\n",
    "categorical_cols_enrolment = ['State', 'Gender', 'Center_Type']\n",
    "df_enrolment_clean = standardize_categorical_columns(\n",
    "    df_enrolment_clean, \n",
    "    categorical_cols_enrolment, \n",
    "    case='title'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize categorical columns in update data\n",
    "categorical_cols_updates = ['Update_Type', 'Update_Reason', 'Previous_State', 'New_State']\n",
    "df_updates_clean = standardize_categorical_columns(\n",
    "    df_updates_clean, \n",
    "    categorical_cols_updates, \n",
    "    case='title'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Create new columns for analysis:\n",
    "1. Age groups from age data\n",
    "2. Biometric quality categories\n",
    "3. Time-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Age Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age groups in enrolment data\n",
    "df_enrolment_clean = create_age_groups(\n",
    "    df_enrolment_clean,\n",
    "    age_column='Age',\n",
    "    new_column_name='Age_Group'\n",
    ")\n",
    "\n",
    "# Visualize age group distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "age_dist = df_enrolment_clean['Age_Group'].value_counts().sort_index()\n",
    "age_dist.plot(kind='bar', color='steelblue')\n",
    "plt.title('Distribution of Enrolments by Age Group', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Age Group', fontsize=12)\n",
    "plt.ylabel('Number of Enrolments', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Age groups created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Biometric Quality Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create biometric quality categories\n",
    "df_enrolment_clean = create_biometric_quality_categories(\n",
    "    df_enrolment_clean,\n",
    "    quality_column='Biometric_Quality_Score',\n",
    "    new_column_name='Quality_Category'\n",
    ")\n",
    "\n",
    "# Visualize quality distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "quality_dist = df_enrolment_clean['Quality_Category'].value_counts().sort_index()\n",
    "quality_dist.plot(kind='bar', color='coral')\n",
    "plt.title('Distribution of Biometric Quality Scores', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Quality Category', fontsize=12)\n",
    "plt.ylabel('Number of Enrolments', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-based features from enrolment date\n",
    "df_enrolment_clean['Enrolment_Year'] = df_enrolment_clean['Enrolment_Date'].dt.year\n",
    "df_enrolment_clean['Enrolment_Month'] = df_enrolment_clean['Enrolment_Date'].dt.month\n",
    "df_enrolment_clean['Enrolment_Quarter'] = df_enrolment_clean['Enrolment_Date'].dt.quarter\n",
    "\n",
    "# Extract time-based features from update date\n",
    "df_updates_clean['Update_Year'] = df_updates_clean['Update_Date'].dt.year\n",
    "df_updates_clean['Update_Month'] = df_updates_clean['Update_Date'].dt.month\n",
    "df_updates_clean['Update_Quarter'] = df_updates_clean['Update_Date'].dt.quarter\n",
    "\n",
    "print(\"✓ Time-based features created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cleaning summary for enrolment data\n",
    "enrolment_summary = get_cleaning_summary(df_enrolment, df_enrolment_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cleaning summary for update data\n",
    "update_summary = get_cleaning_summary(df_updates, df_updates_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned enrolment data\n",
    "save_processed_data(\n",
    "    df_enrolment_clean,\n",
    "    file_name='enrolment_cleaned.csv',\n",
    "    output_dir='../data/processed'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned update data\n",
    "save_processed_data(\n",
    "    df_updates_clean,\n",
    "    file_name='updates_cleaned.csv',\n",
    "    output_dir='../data/processed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL CLEANED DATASETS - READY FOR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nEnrolment Dataset:\")\n",
    "print(f\"  Records: {len(df_enrolment_clean):,}\")\n",
    "print(f\"  Columns: {len(df_enrolment_clean.columns)}\")\n",
    "print(f\"  Date Range: {df_enrolment_clean['Enrolment_Date'].min()} to {df_enrolment_clean['Enrolment_Date'].max()}\")\n",
    "print(f\"  Age Groups: {df_enrolment_clean['Age_Group'].nunique()}\")\n",
    "\n",
    "print(\"\\nUpdate Dataset:\")\n",
    "print(f\"  Records: {len(df_updates_clean):,}\")\n",
    "print(f\"  Columns: {len(df_updates_clean.columns)}\")\n",
    "print(f\"  Date Range: {df_updates_clean['Update_Date'].min()} to {df_updates_clean['Update_Date'].max()}\")\n",
    "print(f\"  Update Types: {df_updates_clean['Update_Type'].nunique()}\")\n",
    "\n",
    "print(\"\\n✓ Data loading and cleaning complete!\")\n",
    "print(\"✓ Ready for exploratory data analysis and statistical modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Exploratory Data Analysis** (Notebook 02): Analyze patterns in biometric updates\n",
    "2. **Statistical Analysis** (Notebook 03): Test hypotheses about age-quality relationships\n",
    "3. **Visualization** (Notebook 04): Create charts for the final report\n",
    "4. **Insights Extraction** (Notebook 05): Derive actionable recommendations for UIDAI\n",
    "\n",
    "---\n",
    "\n",
    "**UIDAI Data Hackathon 2026** | Backend Analytics Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
